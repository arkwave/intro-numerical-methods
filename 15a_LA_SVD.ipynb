{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<table>\n",
    " <tr align=left><td><img align=left src=\"./images/CC-BY.png\">\n",
    " <td>Text provided under a Creative Commons Attribution license, CC-BY. All code is made available under the FSF-approved MIT license. (c) Kyle T. Mandli and Rajath Kumar Mysore Pradeep Kumar</td>\n",
    "</table>\n",
    "\n",
    "Note:  This material largely follows the text \"Numerical Linear Algebra\" by Trefethen and Bau (SIAM, 1997) and is meant as a guide and supplement to the material presented there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%precision 6 \n",
    "%matplotlib inline\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Understanding the Singular Value Decomposition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Factorizations:  A review\n",
    "\n",
    "**Matrix Factorization** is a fundamental concept in Linear Algebra and refers to the ability to write general matrices as the products of simpler matrices with special properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In particular, all of the great algorithms encountered in numerical linear Algebra can be succinctly described by their factorizations.  Examples include"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### **The LU decomposition**  \n",
    "$$PA=LU$$\n",
    "    \n",
    "* **Algorithms** *Gaussian Elimination with partial pivoting* \n",
    "* **Factorization** $P$ Permutation matrix (from pivoting), $L,U$: Lower and upper triangular matrices\n",
    "* **Application** direct solution of $A\\mathbf{x}=\\mathbf{b}$ for $A\\in\\mathbb{R}^{n\\times n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### **The QR decomposition**  \n",
    "$$A=QR$$\n",
    "    \n",
    "* **Algorithms** Gram-Schmidt Orthogonalization, Householder Triangularization \n",
    "* **Factorization**  $Q$:  Orthonormal Basis for $C(A)$, $R$: upper triangular matrix\n",
    "* **Applications** Least-Squares solutions, Projection problems,  Eigenvalues $QR/RQ$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### **EigenProblems**  \n",
    "$$ AX = X\\Lambda, \\quad (A\\in\\mathbb{R}^{n\\times n})$$\n",
    "\n",
    "* Diagonalizable matrices $A = X\\Lambda X^{-1}$\n",
    "* Hermitian/Symmetric Matrices $A = Q\\Lambda Q^T$\n",
    "* Schur Factorization $A = QTQ^T$\n",
    "    \n",
    "    \n",
    "* **Algorithms**: Power Method, Inverse Power with shifts,  $QR/RQ$\n",
    "* **Factorization**-- $X$, $Q$: matrix of Eigenvectors,  $\\Lambda$ and diagonal matrix of eigenvalues \n",
    "* **Application** Solution of Dynamical systems,  Iterative Maps, Markov Chains,  Vibrational analysis$\\ldots$, Quantum Mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*But perhaps the most beautiful factorization, which contains aspects of all of these problems$\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Singular Value Decomposition (SVD)\n",
    "$$\n",
    "    A = U\\Sigma V^{T}\n",
    "$$\n",
    "for *any* $A\\in\\mathbb{R}^{m\\times n}$, where\n",
    "* $U \\in \\mathbb R^{m \\times m}$ and is the orthogonal matrix whose columns are the eigenvectors of $AA^{T}$\n",
    "* $V \\in \\mathbb R^{n \\times n}$ and is the orthogonal matrix whose columns are the eigenvectors of $A^{T}A$\n",
    "* $\\Sigma \\in \\mathbb R^{m \\times n}$ and is a diagonal matrix with elements $\\sigma_{1}, \\sigma_{2}, \\sigma_{3}, ... \\sigma_{r}$ where $r = rank(A)$ corresponding to the square roots of the eigenvalues of $A^{T}A$. They are called the singular values of $A$ and are positive arranged in descending order. ($\\sigma_{1} \\geq \\sigma_{2} \\geq \\sigma_{3} \\geq ... \\sigma_{r} > 0$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or a picture is worth a thousand words...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Existence and Uniqueness\n",
    "\n",
    "Every matrix $A \\in \\mathbb{C}^{m \\times n}$ has a singular value decomposition. Furthermore, the singular values $\\{\\sigma_{j}\\}$ are uniquely determined, and if $A$ is square and the $\\sigma_{j}$ are distinct, the left and right singular vectors $\\{u_{j}\\}$ and $\\{v_{j}\\}$ are uniquely determined up to complex signs (i.e., complex scalar factors of absolute value 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Eigenvalue Decomposition vs. SVD Decomposition\n",
    "\n",
    "Let the matrix $X$ contain the eigenvectors of $A$ which are linearly independent, then we can write a decomposition of the matrix $A$ as\n",
    "$$\n",
    "    A = X \\Lambda X^{-1}.\n",
    "$$\n",
    "\n",
    "How does this differ from the SVD?\n",
    " - The basis of the SVD representation differs from the eigenvalue decomposition\n",
    "   - The basis vectors are not in general orthogonal for the eigenvalue decomposition where it is for the SVD\n",
    "   - The SVD effectively contains two basis sets.\n",
    " - All matrices have an SVD decomposition whereas not all have eigenvalue decompositions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Applications\n",
    "* Matrix Pseudo-Inverse for ill-conditions problems\n",
    "* Principal Component analysis\n",
    "* Total Least Squares\n",
    "* Image Compression\n",
    "* Data Analysis for interpretation of high-dimensional data\n",
    "* and more$\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A conceptual computation of the SVD (not how it's really done)\n",
    "\n",
    "Given $A = U\\Sigma V^T$, we can form the matrix \n",
    "\n",
    "$$\n",
    "    A^TA = V\\Sigma^T\\Sigma V^T\n",
    "$$\n",
    "\n",
    "where $V\\in\\mathbb{R}^{n\\times n}$ is unitary and\n",
    "$$\n",
    "    \\Sigma^T\\Sigma = \\begin{bmatrix} \n",
    "                \\sigma_1^2 & & &    \\\\\n",
    "                    & \\sigma_2^2 & & \\\\\n",
    "                   & & \\ddots &   \\\\\n",
    "                   & & &   \\sigma_n^2  \\\\ \n",
    "                                   \\end{bmatrix}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, because $A^TA$ is clearly **Symmetric** it can also has the eigen factorization\n",
    "\n",
    "$$\n",
    "    A^TA = Q\\Lambda Q^T\n",
    "$$\n",
    "\n",
    "where $Q$ is unitary and $\\Lambda$ is real diagonal.  \n",
    "\n",
    "Moreover, we can show that $A^T A$ is at least Positive semi-definite so all the eigenvalues are $\\geq 0$. \n",
    "\n",
    "In particular,  if $\\mathrm{rank}(A)=r$ we know that $r$ eigenvalues are $>0$ and $n-r$ eigenvalues are equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given that\n",
    "\n",
    "$$\\begin{align}\n",
    "    A^TA &= V\\Sigma^T\\Sigma V^T \\\\\n",
    "        &= Q\\Lambda Q^T \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If we simply order the eigenvalues from largest to smallest (and rearrange the columns of $Q$ to match), then we can make the association\n",
    "\n",
    "$$V = Q\\quad, \\Sigma^T\\Sigma = \\Lambda$$ or\n",
    "\n",
    "$$\\sigma_i^2 = \\lambda_i\\quad\\textrm{or}\\quad \\sigma_i = \\sqrt{\\lambda_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A similar argument can be made about $U$ and the eigenvectors of $AA^T$,  however, a better way to think about the relationship between $U$, $V$ and $\\Sigma$ is to rearrange the SVD as\n",
    "\n",
    "$$ \n",
    "    AV = U\\Sigma\n",
    "$$\n",
    "\n",
    "or we can look at this column-by-column \n",
    "\n",
    "$$\n",
    "    A\\mathbf{v}_i = \\sigma_i\\mathbf{u}_i\n",
    "$$\n",
    "\n",
    "(in the same way the Eigen decomposition $AX = X\\Lambda$ is equivalent to $A\\mathbf{x}_i = \\lambda_i\\mathbf{x}_i$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "or rearranging \n",
    "$$\n",
    "    \\mathbf{u}_i = \\frac{A\\mathbf{v}_i}{\\sigma_i},\\quad i=1,2,\\ldots,r\n",
    "$$\n",
    "lets us solve for the first $r$ columns of $U$ given $V$ and $\\Sigma$.  \n",
    "\n",
    "As we'll show, we usually don't need to solve for the remaing $m-r$ columns of $U$ for reasons that relate to the \"4 fundamental subspaces of $A$\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Strangian view of the 4 Subspaces: a quick refresher\n",
    "\n",
    "<img align=center src=\"./images/Strang_4_subspaces.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Big idea:  The columns of $U$ and $V$ contain orthonormal bases for the 4 fundamental subspaces\n",
    "\n",
    "Returning to $AV = U\\Sigma$ it follows that \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "        A\\mathbf{v}_i &= \\sigma_i\\mathbf{u_i},\\quad\\mathrm{for}\\quad i=1,2\\ldots r \\\\\n",
    "        A\\mathbf{v}_i &= \\mathbf{0},\\quad\\mathrm{for}\\quad i=r+1,\\ldots n \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Therefore the last $n-r$ columns of $V$ must be in the Null space of $A$, and in fact must form an orthonormal basis for the Null space $N(A)$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since the first $r$ columns of $V$ are all orthogonal to the last $n-r$ columns,  they must form an orthonormal basis for the row space $C(A^T)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Likewise,  since the first $r$ columns of $U$ satisfy\n",
    "\n",
    "$$\n",
    "    \\mathbf{u}_i = \\frac{A\\mathbf{v}_i}{\\sigma_i},\\quad i=1,2,\\ldots,r\n",
    "$$\n",
    "\n",
    "they must form an orthonormal basis for ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Leaving only the last $m-r$ columns of $U$ as an orthonormal basis for the left null space $N(A^T)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again a picture is worth a lot here\n",
    "\n",
    "$$\n",
    "A\\begin{bmatrix} &  &  & |&  &  &  \\\\\n",
    " &  &  & |&  &  &  \\\\\n",
    " \\mathbf{v}_1 & \\cdots & \\mathbf{v}_r & | &\\mathbf{v}_{r+1} & \\cdots & \\mathbf{v}_n \\\\\n",
    " &  &  & |&  &  &  \\\\\n",
    "  &  &  & |&  &  &  \\\\\n",
    "   \\end{bmatrix} = \\begin{bmatrix} &  &  & |&  &  &  \\\\\n",
    " &  &  & |&  &  &  \\\\\n",
    " &  &  & |&  &  &  \\\\\n",
    " \\mathbf{u}_1 & \\cdots & \\mathbf{u}_r & | &\\mathbf{u}_{r+1} & \\cdots & \\mathbf{u}_m \\\\\n",
    " &  &  & |&  &  &  \\\\\n",
    " &  &  & |&  &  &  \\\\\n",
    "  &  &  & |&  &  &  \\\\\n",
    "    \\end{bmatrix} \\begin{bmatrix} \\sigma_1 &  &  & |  &  &    \\\\\n",
    " & \\ddots &  & |   &   \\mathbf{0}  \\\\\n",
    " &  & \\sigma_r &|    &     \\\\\n",
    " -&- &-  & | & - &  -\\\\\n",
    " & \\mathbf{0} &  & |   & \\mathbf{0}   \\\\\n",
    " &  &  & |  &    \\\\\n",
    "  &  &  & |  &     \\\\\n",
    "    \\end{bmatrix} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Economy (or Skinny) SVD\n",
    "\n",
    "As it turns out, because of all the 0's  all we actually need to reconstruct $A$ is the the first $r$ columns of $U$, and $V$,  and the square sub-block of $\\Sigma$ with just the singular values.  i.e.\n",
    "\n",
    "$$\n",
    "A\\begin{bmatrix} &  &  & \\\\\n",
    " &  &  &    \\\\\n",
    " \\mathbf{v}_1 & \\cdots & \\mathbf{v}_r   \\\\\n",
    " &  &  &   \\\\\n",
    "  &  &  &    \\\\\n",
    "   \\end{bmatrix} = \\begin{bmatrix} &  &  &   \\\\\n",
    " &  &  &    \\\\\n",
    "  &  &  &    \\\\ \n",
    " \\mathbf{u}_1 & \\cdots & \\mathbf{u}_r     \\\\\n",
    " &  &  &    \\\\\n",
    " &  &  &    \\\\\n",
    "  &  &  &    \\\\\n",
    "    \\end{bmatrix} \\begin{bmatrix} \\sigma_1 &  &        \\\\\n",
    " & \\ddots &    \\\\\n",
    " &  & \\sigma_r       \\\\\n",
    "    \\end{bmatrix} \n",
    "$$\n",
    "or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "    A = U_r\\Sigma_r V_r^T\n",
    "$$\n",
    "\n",
    "And it is this object (The Economy (or Skinny) SVD), that helps us understand what the SVD does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Spectral Theorem\n",
    "\n",
    "The Economy SVD can also be written as \n",
    "$$\n",
    "    A = U_r\\Sigma_r V_r^T = \\sum_{i=1}^r \\sigma_i \\mathbf{u}_i\\mathbf{v}^T_i\n",
    "$$\n",
    "\n",
    "Which says we can expand $A$ as a series of rank-1 matrices $\\mathbf{u}_i\\mathbf{v}^T_i$ weighted by the singular values...\n",
    "\n",
    "and it is this picture that leads to many of the important approximating properties of the SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix Multiplication \n",
    "\n",
    "Consider the Matrix Vector product $A\\mathbf{x}$ which maps a vector $\\mathbf{x}$ in the row space $C(A^T)$ to its image in column space $C(A)$.  In the context of the Economy SVD\n",
    "$$\n",
    "    A\\mathbf{x} = U_r\\Sigma_r V_r^T\\mathbf{x}\n",
    "$$\n",
    "or (since $V_r^TV_r = I$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "    A\\mathbf{x} &= U_r\\Sigma_r V_r^T\\mathbf{x} \\\\\n",
    "    &= U_r\\Sigma_r (V_r^T V_r) V_r^T\\mathbf{x} \\\\\n",
    "    & = (U_r\\Sigma_r V_r^T) (V_r V_r^T)\\mathbf{x} \\\\\n",
    "    & = A \\mathbf{x}^+ \n",
    "\\end{align}\n",
    "$$\n",
    "where $$\\mathbf{x}^+ = (V_r V_r^T)\\mathbf{x}$$\n",
    "is ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Question**: If we wanted it, how would we  find the projection of $\\mathbf{x}$ onto the Null space of $A$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SVD and the Matrix Inverse \n",
    "\n",
    "If a matrix is square and invertible, it implies that $m=n=r$ and $U$, $\\Sigma$ and $V$ are all square invertible matrices,  therefore if \n",
    "\n",
    "$$\n",
    "    A = U\\Sigma V^T\n",
    "$$\n",
    "then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "    A^{-1} = V\\Sigma^{-1} U^T\n",
    "$$\n",
    "where \n",
    "$$\n",
    "    \\Sigma^{-1} = \\begin{bmatrix} 1/\\sigma_1 &  &    &    \\\\\n",
    " & 1/\\sigma_2 &   & \\\\\n",
    "  &   &  \\ddots &  \\\\\n",
    " &  &  & 1/\\sigma_n       \\\\\n",
    "    \\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "and clearly \n",
    "$$\n",
    "    A^{-1}\\mathbf{b} = \\mathbf{x}\n",
    "$$ \n",
    "maps any vector $\\mathbf{b}\\in C(A)$ back to a unique vector $\\mathbf{x}\\in C(A^T)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SVD and the Matrix Pseudo-Inverse \n",
    "\n",
    "But suppose $A$ is not square.  Then it cannot have an inverse,  however, the SVD allows us to define a \"Pseudo-Inverse\"\n",
    "\n",
    "Given, the skinny SVD\n",
    "\n",
    "$$\n",
    "    A = U_r\\Sigma_r V_r^T\n",
    "$$\n",
    "\n",
    "we can define\n",
    "\n",
    "$$\n",
    "    A^{+} = V_r \\Sigma_r^{-1} U_r^T\n",
    "$$\n",
    "because $\\Sigma_r$ is square and invertible (but it's size is set by the rank $r$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Action of the  Pseudo-Inverse \n",
    "\n",
    "In general, if $A$ is not square,  the problem $A\\mathbf{x} = \\mathbf{b}$ has either no solution or an infinite number of solutions.  However, it always has a minimal least square's solution given by \n",
    "$$\n",
    "    x^{+} = A^{+}\\mathbf{b}\n",
    "$$\n",
    "which maps any vector $\\mathbf{b}\\in C(A)$ to a unique vector $\\mathbf{x}^+\\in C(A^T)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we did with $A\\mathbf{x}$, we can consider the action of because $A^{+}$ on any vector $\\mathbf{b}$ as a projection problem.\n",
    "$$\n",
    "\\begin{align}\n",
    "    A^{+}\\mathbf{b} &= V_r\\Sigma^{-1}_r U_r^T\\mathbf{x} \\\\\n",
    "    &= V_r\\Sigma^{-1}_r (U_r^T U_r) U_r^T\\mathbf{x} \\\\\n",
    "    & = (V_r\\Sigma^{-1}_r U_r^T) (U_r U_r^T)\\mathbf{x} \\\\\n",
    "    & = A^{+} \\mathbf{p}  \n",
    "\\end{align}\n",
    "$$\n",
    "where $$\\mathbf{p}  = (U_r U_r^T)\\mathbf{b}$$\n",
    "is ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The minimal Least-Squares solution\n",
    "\n",
    "It should be clear that \n",
    "$$\n",
    "    \\mathbf{x}^+ = A^{+}\\mathbf{b}\n",
    "$$ \n",
    "lies entirely in the Row space of $A$ (Why?)\n",
    "\n",
    "Therefore it has no component in the $N(A)$ and is the shortest solution to $A\\hat{\\mathbf{x}}=\\mathbf{p}$ which is the least squares problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Pseudo-Inverse and ill-conditioned problems\n",
    "\n",
    "Technically, a square matrix is either invertible or singular,  however, in many real problems, a matrix can be close to singular or \"ill-conditioned\".\n",
    "\n",
    "Again, the condition number of a matrix defined by an induced $p$ norm is\n",
    "$$\n",
    "    \\kappa_p(A) = ||A||_p ||A^{-1}||_p\n",
    "$$\n",
    "\n",
    "In particular,  for $p=2$, it is easy to show (using the SVD) that for a square matrix\n",
    "$$\n",
    "    \\kappa_2(A) =\\frac{\\sigma_1}{\\sigma_n}\n",
    "$$\n",
    "\n",
    "and thus for a singular matrix with $r<n$, $\\kappa_2 =\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  ill-conditioned matrices\n",
    "\n",
    "However, it is possible that while $\\sigma_n>0$,  it is significantly smaller than $\\sigma_1$, leading to a very large condition number.  In particular if the last $j$ singular values are very small, it suggest that the matrix has a \"near Null Space\",  with a basis defined by the last $j$ columns of $V$.  Thus, while strictly invertible,  if a direct solver is used to solve\n",
    "$$\n",
    "A\\mathbf{x} =\\mathbf{b}\n",
    "$$ \n",
    "any component in the near null space will be amplified by $1/\\sigma_j$ and completely pollute your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, a useful fix can be to approximate $A$ as a lower rank matrix \n",
    "\n",
    "$$A_k = U_k\\Sigma_kV_k^T$$\n",
    "\n",
    "which just keeps the first $k$ singular values (i.e. pretends $r=k$).  If chosen wisely,  The approximate solution\n",
    "$$\n",
    "    \\mathbf{x^{+} = A^{+}_k\\mathbf{b}\n",
    "$$ \n",
    "can be considerably more accurate as it does not allow the near-null space to contaminate the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Full SVD example\n",
    "\n",
    "Consider the matrix\n",
    "$$\n",
    "    A = \\begin{bmatrix} \n",
    "        2 & 0 & 3 \\\\\n",
    "        5 & 7 & 1 \\\\\n",
    "        0 & 6 & 2 \n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "Confirm the SVD representation using `numpy` functions as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "A = numpy.array([\n",
    "    [2.0, 0.0, 3.0],\n",
    "    [5.0, 7.0, 1.0],\n",
    "    [0.0, 6.0, 2.0]\n",
    "])\n",
    "\n",
    "U, sigma, V_T = numpy.linalg.svd(A, full_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('U:\\n{}\\n'.format(U))\n",
    "print('Sigma:\\n{}\\n'.format(numpy.diag(sigma)))\n",
    "print('V:\\n{}\\n'.format(V_T.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Aprime = numpy.dot(U, numpy.dot(numpy.diag(sigma), V_T))\n",
    "print('A - USV^T:\\n{}'.format(A - Aprime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: a rank-1 Matrix \n",
    "\n",
    "$$A = \\mathbf{x}\\mathbf{y}^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x = numpy.array([ 1., 2., 1. ])\n",
    "y = numpy.array([2., -1., 1. ])\n",
    "A = numpy.outer(x,y)\n",
    "print('A:\\n{}\\n'.format(A))\n",
    "U, sigma, V_T = numpy.linalg.svd(A, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print('U:\\n{}\\n'.format(U))\n",
    "print('sigma:\\n{}\\n'.format(numpy.diag(sigma)))\n",
    "print('V:\\n{}\\n'.format(V_T.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Aprime = numpy.dot(U, numpy.dot(numpy.diag(sigma), V_T))\n",
    "print('A - USV^T:\\n{}'.format(A - Aprime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# rank-1 reconstruction\n",
    "k = 1\n",
    "A1 = numpy.dot(U[:,:k], numpy.dot(numpy.diag(sigma[:k]), V_T[:k,:]))\n",
    "print('A_k =\\n{}\\n'.format(A1))\n",
    "print('A - A_k:\\n{}'.format(A - A1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An ill-conditioned example\n",
    "\n",
    "Consider the matrix\n",
    "$$\n",
    "    A = \\begin{bmatrix} \n",
    "        1 & 0 & 1 \\\\\n",
    "        1 & 1 & 2+\\epsilon \\\\\n",
    "        0 & 1 & 1\\\\\n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "where the last column is almost the sum of the first two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "epsilon = 1.e-3\n",
    "A = numpy.array([\n",
    "    [1.0, 0.0, 1.0],\n",
    "    [1.0, 1.0, 2+epsilon],\n",
    "    [0.0, 1.0, 1.0]\n",
    "])\n",
    "\n",
    "U, S, V_T = numpy.linalg.svd(A, full_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('U:\\n{}\\n'.format(U))\n",
    "print('S:\\n{}\\n'.format(numpy.diag(S)))\n",
    "print('V:\\n{}\\n'.format(V_T.T))\n",
    "print('sigma_1/sigma_n = {}, cond_2(A) = {}'.format(S[0]/S[2], numpy.linalg.cond(A, p=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Try to solve $A\\mathbf{x}=\\mathbf{b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "finfo = numpy.finfo(numpy.float64)\n",
    "epsilon = 2*finfo.eps\n",
    "A = numpy.array([\n",
    "    [1.0, 0.0, 1.0],\n",
    "    [1.0, 1.0, 2+epsilon],\n",
    "    [0.0, 1.0, 1.0]\n",
    "])\n",
    "U, S, VT = numpy.linalg.svd(A)\n",
    "x_true = numpy.array([1., 2., 3.])\n",
    "b = A.dot(x_true)\n",
    "x = numpy.linalg.solve(A,b)\n",
    "print('x = {}: cond(A)={}, S ={}'.format(x, numpy.linalg.cond(A, p=2), S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# rank-2 reconstruction\n",
    "k = 2\n",
    "A2 = numpy.dot(U[:,:k], numpy.dot(numpy.diag(S[:k]), VT[:k,:]))\n",
    "#Pseudo Inverse\n",
    "Ap2 = numpy.dot(VT.T[:,:k], numpy.dot(numpy.diag(1./S[:k]), U.T[:k,:]))\n",
    "xp = Ap2.dot(b)\n",
    "print('x = {}: cond(A)={}, S ={}'.format(x, numpy.linalg.cond(A, p=2), S))\n",
    "print('x^+ = {}'.format(xp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matrix Properties via the SVD\n",
    "\n",
    " - The $\\text{rank}(A) = r$ where $r$ is the number of non-zero singular values.\n",
    " - The $\\text{range}(A) = \\mathrm{span}\\langle u_1, ... , u_r\\rangle$ and $\\text{null}(a) = \\mathrm{span}\\langle v_{r+1}, ... , v_n\\rangle$.\n",
    " - The $|| A ||_2 = \\sigma_1$ and $||A||_F = \\sqrt{\\sigma_{1}^{2}+\\sigma_{2}^{2}+...+\\sigma_{r}^{2}}$.\n",
    " - The nonzero singular values of A are the square roots of the nonzero eigenvalues of $A^{T}A$ or $AA^{T}$.\n",
    " - If $A = A^{T}$, then the singular values of $A$ are the absolute values of the eigenvalues of $A$.\n",
    " - For $A \\in \\mathbb{C}^{m \\times m}$ then $|det(A)| = \\Pi_{i=1}^{m} \\sigma_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Low-Rank Approximations\n",
    "\n",
    " - $A$ is the sum of the $r$ rank-one matrices:\n",
    "$$\n",
    "    A = U \\Sigma V^T = \\sum_{j=1}^{r} \\sigma_{j}u_{j}v_{j}^{T}\n",
    "$$\n",
    " - For any $k$ with $0 \\leq k \\leq r$, define\n",
    "$$\n",
    "    A = \\sum_{j=1}^{k} \\sigma_{j}u_{j}v_{j}^{T}\n",
    "$$\n",
    "Let $k = min(m,n)$, then\n",
    "\n",
    "$$\n",
    "    ||A - A_{v}||_{2} = \\text{inf}_{B \\in \\mathbb{C}^{m \\times n}} \\text{rank}(B)\\leq k|| A-B||_{2} = \\sigma_{k+1}\n",
    "$$\n",
    "\n",
    "- For any $k$ with $0 \\leq k \\leq r$, the matrix $A_{k}$ also satisfies\n",
    "$$\n",
    "    ||A - A_{v}||_{F} = \\text{inf}_{B \\in \\mathbb{C}^{m \\times n}} \\text{rank}(B)\\leq v ||A-B||_{F} = \\sqrt{\\sigma_{v+1}^{2} + ... + \\sigma_{r}^{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example:   \"Hello World\"\n",
    "\n",
    "How does this work in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data = numpy.zeros((15,40))\n",
    "m, n = data.shape\n",
    "print('{}x{} pixel image'.format(m,n))\n",
    "\n",
    "\n",
    "#H\n",
    "data[2:10,2:4] = 1\n",
    "data[5:7,4:6] = 1\n",
    "data[2:10,6:8] = 1\n",
    "\n",
    "#E\n",
    "data[3:11,10:12] = 1\n",
    "data[3:5,12:16] = 1\n",
    "data[6:8, 12:16] = 1\n",
    "data[9:11, 12:16] = 1\n",
    "\n",
    "#L\n",
    "data[4:12,18:20] = 1\n",
    "data[10:12,20:24] = 1\n",
    "\n",
    "#L\n",
    "data[5:13,26:28] = 1\n",
    "data[11:13,28:32] = 1\n",
    "\n",
    "#0\n",
    "data[6:14,34:36] = 1\n",
    "data[6:8, 36:38] = 1\n",
    "data[12:14, 36:38] = 1\n",
    "data[6:14,38:40] = 1\n",
    "\n",
    "plt.imshow(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "u, s, vt = numpy.linalg.svd(data)\n",
    "print(u.shape, s.shape, vt.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.semilogy(s,'bo-')\n",
    "axes.set_ylabel('$\\sigma$', fontsize=16)\n",
    "axes.grid()\n",
    "axes.set_title('Spectrum of Singular Values')\n",
    "plt.show()\n",
    "print('s = {}'.format(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(fig.get_figwidth() * 3)\n",
    "fig.set_figheight(fig.get_figheight() * 4)\n",
    "for i in range(m):\n",
    "    mode = s[i]*numpy.outer(u[:,i], vt[i,:])\n",
    "    \n",
    "    axes = fig.add_subplot(5, 3, i+1)\n",
    "    mappable = axes.imshow(mode, vmin=0.0, vmax=1.0)\n",
    "    axes.set_title('mode$_{{{}}}$'.format(i+1), fontsize=18)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(fig.get_figwidth() * 3)\n",
    "fig.set_figheight(fig.get_figheight() * 4)\n",
    "for k in range(1,16):\n",
    "    Ak = u[:,:k].dot(numpy.diag(s[:k]).dot(vt[:k,:]))\n",
    "    axes = fig.add_subplot(5, 3, k)\n",
    "    mappable = axes.imshow(Ak, vmin=0.0, vmax=1.0)\n",
    "    axes.set_title('$A_k$, $k={}$'.format(k))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A bigger example:  the original Matrix (From Durer's Melancholia)\n",
    "\n",
    "<img align=center src=\"./images/Durer_Melancholia_I.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import image\n",
    "data = image.imread('images/melancholia-magic-square.png')\n",
    "m,n = data.shape\n",
    "print('{}x{} pixel image'.format(m,n))\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(data,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "u, s, vt = numpy.linalg.svd(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.semilogy(s,'bo-')\n",
    "axes.set_ylabel('$\\sigma$', fontsize=16)\n",
    "axes.grid()\n",
    "axes.set_title('Spectrum of Singular Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### First 4 Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(fig.get_figwidth() * 4)\n",
    "fig.set_figheight(fig.get_figheight() * 1)\n",
    "for i in range(4):\n",
    "    mode = s[i]*numpy.outer(u[:,i], vt[i,:])\n",
    "\n",
    "    axes = fig.add_subplot(1, 4, i+1)\n",
    "    mappable = axes.imshow(mode, cmap='gray')\n",
    "    axes.set_title('Mode = {}, $\\sigma$={:3.3f}'.format(i+1,s[i]),fontsize=18)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(fig.get_figwidth() * 3)\n",
    "fig.set_figheight(fig.get_figheight() *2)\n",
    "for i,k in enumerate([1, 10, 20, 50, 100, 200]):\n",
    "    Ak = u[:,:k].dot(numpy.diag(s[:k]).dot(vt[:k,:]))\n",
    "    storage = 100.*k*(m+n + 1)/(m*n)\n",
    "    axes = fig.add_subplot(2, 3, i+1)\n",
    "    mappable = axes.imshow(Ak, vmin=0.0, vmax=1.0, cmap='gray')\n",
    "    axes.set_title('$A_{{{}}}$, $storage={:2.2f}\\%$'.format(k,storage),fontsize=16)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other Applications\n",
    "\n",
    "* Total Least-squares\n",
    "* PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
